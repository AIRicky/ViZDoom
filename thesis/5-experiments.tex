\chapter{Experiment}

\section{Goal of the Experiment}
The main purpose of the experiment is to show that Reinforcement Learning from visual input is possible using Vizia Environment. 	
Additionally, the eperiments tries to research influence of frame skipping on learning process.

\section{Tested Agent Design}
	The agent is heavily inspired by Google DeepMind Atari DQN paper\cite{mnih-dqn-2015} and is conceptually very similar to DeepMind's code. Code is written in Python and uses Theano\cite{Bastien-Theano-2012}\cite{bergstra+al:2010-scipy} and Lasagne\cite{sander_dieleman_2015_27878} to implement neural networks.
	\\
	The agent is divided into two separate modules:

	\subsubsection{Q-learning engine:} 
	\subsubsection{Q Value Approximator:} 
	\begin{itemize}
		\item python, theano, lasagne
		\item Q learning, convolutional neural network, eps-greedy policy with linear epsilon decay, action replay
		\item pseudo code here? (ommit if short on time)
	\end{itemize}

\section{Experimental Settings} 

	\subsection{Operating System and Hardware}
	\begin{description}
		\item[Operating System] Linux Mint 17 x86\_64, kernel 3.13.0-24-generic
		\item[CPU] Intel Core i7-4790, 4x4GHz
		\item[GPU] GeForce GTX 970, 1664 CUDA cores, 4GB RAM
	\end{description}

	\subsection{Game Settings}
		The experiment uses the most basic scenario from the pool, that is `basic' scenario (see \ref{subsec:basic}). Screen buffer consists of 3 channels (RGB) and resolution is 60x45 pixels.

	\subsection{Neural Network Architecture}
		network architecture used in the experiment, fancy diagram of this architecture here? (there are 2 conv and 2 mlp layers so it can be drawn and still make sense)
	\subsection{Agent Parameters}

\section{Results}
Graphs and conclusions . . .